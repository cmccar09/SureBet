# V2.3: Comparative Learning - All Horses Analysis

**Implementation Date:** January 31, 2026  
**Status:** ‚úÖ COMPLETE  
**Purpose:** Store predictions for ALL horses analyzed per race to enable "why did winner beat our pick?" analysis

---

## üéØ Problem Statement

**Previously:** System only stored predictions for SELECTED horses  
**Issue:** Cannot analyze why actual winners scored lower than our losing picks  
**Example:** 16:55 Newcastle - picked Water Of Leith (4th place), Pal Joey won at 8/1
- ‚ùå Without winner's data: "We were wrong"
- ‚úÖ With winner's data: "We underestimated Pal Joey because we overweighted recent form vs class advantage"

---

## üîß Implementation

### 1. Enhanced Analysis Output (enhanced_analysis.py)

**Updated 3 Expert Analysis Functions:**

```python
# Each expert now returns:
{
  "all_horses": [
    # ALL horses in race with predictions
    {
      "runner_name": "Horse Name",
      "selection_id": "12345",
      "value_score": 8,
      "true_probability": 0.28,
      "reasoning": "Brief analysis"
    }
  ],
  "selections": [
    # Top 2-4 picks only
    {...}
  ]
}
```

**Modified Functions:**
- `analyze_value_angle()` - Returns all horses with value scores
- `analyze_form_angle()` - Returns all horses with form scores
- `analyze_class_drop_angle()` - Returns all horses with advantage scores

**Race Analysis Enhancement:**
```python
# Collects all horses from 3 expert analyses
all_horses_data = {
    'value_analysis': value_result.get('all_horses', []),
    'form_analysis': form_result.get('all_horses', []),
    'class_analysis': class_result.get('all_horses', [])
}

# Attached to each selected pick
sel['all_horses_analyzed'] = all_horses_data
```

### 2. Storage Expansion (save_selections_to_dynamodb.py)

**New Field in DynamoDB:**
```python
bet_item = {
    # ... existing fields ...
    
    # NEW: All horses analyzed for this race
    'all_horses_analyzed': bet.get('all_horses_analyzed', {}),
}
```

**Data Stored Per Race:**
- Value expert's ratings for ALL runners
- Form expert's ratings for ALL runners  
- Class expert's ratings for ALL runners
- Enables post-race comparison of any horse vs our pick

### 3. Comparative Analysis (analyze_prediction_calibration.py)

**New Functions:**

#### `compare_pick_vs_winner(pick)`
Compares our losing pick vs actual winner across all 3 expert dimensions:

```python
comparison = {
    'our_pick': 'Water Of Leith',
    'actual_winner': 'Pal Joey',
    'analyses': {
        'value': {
            'our_pick': {'true_probability': 0.35, 'value_score': 8},
            'winner': {'true_probability': 0.22, 'value_score': 5},
            'differential': {'probability_gap': +0.13, 'value_gap': +3}
        },
        'form': {...},
        'class': {...}
    },
    'insights': [
        "Overestimated Water Of Leith probability by 13%",
        "Rated Water Of Leith as much better value (+3 points) than winner",
        "Missed Pal Joey's class advantage (underrated by 4 points)"
    ]
}
```

#### `analyze_all_losses_vs_winners(days_back=7)`
Aggregates all losing bets to find systematic patterns:

```python
patterns = {
    'total_losses_analyzed': 15,
    'common_mistakes': {
        'probability_overestimation': 8,  # Count of occurrences
        'form_overweight': 5,
        'class_underweight': 6,
        'value_misjudgment': 4
    },
    'recommendations': [
        "CRITICAL: Consistently overestimating win probabilities - recalibrate downward",
        "WARNING: Over-relying on recent form - increase weight on class/conditions"
    ]
}
```

**Output Files:**
- `calibration_report.json` - Prediction accuracy bins
- `loss_comparison_analysis.json` - NEW: Detailed winner vs pick comparisons

---

## üìä Analysis Workflow

### Before (V2.2):
1. Make picks ‚Üí Save selected horses only
2. Race settles ‚Üí Record outcome (win/loss)
3. Calibration analysis ‚Üí "We overestimated probability"
4. Learning ‚Üí Generic "be more conservative"

### After (V2.3):
1. Make picks ‚Üí Save ALL horses analyzed
2. Race settles ‚Üí Record outcome + winner name
3. Calibration analysis ‚Üí "We overestimated probability"
4. **Comparative analysis** ‚Üí "We overweighted form, missed class advantage"
5. Learning ‚Üí **Specific "reduce form weight by 20%, increase class weight by 15%"**

---

## üéØ Learning Insights Enabled

### Question: "Why did we lose?"

**V2.2 Answer:**
- We predicted 35% probability
- Actual win rate in that bin: 20%
- Conclusion: Overconfident

**V2.3 Answer:**
- We predicted 35% for Water Of Leith
- Pal Joey (winner) we rated at 22%
- Value analysis: We rated our pick +3 points higher
- Form analysis: We rated our pick +2 points higher
- Class analysis: Winner was +4 points better (WE MISSED THIS)
- **Conclusion: Overweighted recent form, underweighted class advantage**

### Question: "What pattern causes losses?"

**V2.2 Answer:**
- 20-40% confidence bin has 20% win rate (overconfident)
- Longshots (8+/1) have 0% win rate (avoid)

**V2.3 Answer:**
- **8 of 15 losses:** We overestimated our pick's probability by 10-15%
- **5 of 15 losses:** We overweighted recent form vs actual winner
- **6 of 15 losses:** We underrated winners with strong class advantages
- **ACTIONABLE:** Reduce form scoring by 20%, increase class scoring by 15%

---

## üîÑ Integration with Existing Systems

### Daily Learning Cycle:
```
scheduled_workflow.ps1 (every 30 min)
  ‚Üì
fetch races ‚Üí enhanced_analysis.py (analyzes ALL horses)
  ‚Üì
save_selections_to_dynamodb.py (stores all_horses_analyzed)
  ‚Üì
LATER: Race settles ‚Üí betfair_results_fetcher_v2.py
  ‚Üì
generate_learning_insights.py (v2.2 calibration)
  ‚Üì
analyze_prediction_calibration.py (v2.3 comparative analysis)
  ‚Üì
learning_insights.json ‚Üí Updated with specific pattern fixes
```

### Storage Impact:
- **Before:** 1 pick = ~2KB (selected horse data)
- **After:** 1 pick = ~5-8KB (selected + all horses analyzed)
- **10 runners/race:** Additional ~3-6KB per pick
- **Cost:** Minimal (DynamoDB charges by read/write, not size under 400KB)

---

## ‚úÖ Testing Plan

### Phase 1: Validation (Next Workflow Run)
1. Trigger workflow manually
2. Verify `all_horses_analyzed` field populated in DynamoDB
3. Check 3 expert analyses present (value, form, class)
4. Confirm all runners in race included

### Phase 2: Post-Race Analysis (After Settlements)
1. Wait for races to settle (betfair_results_fetcher_v2.py)
2. Run `python analyze_prediction_calibration.py`
3. Verify `loss_comparison_analysis.json` generated
4. Review comparative insights for losing bets

### Phase 3: Learning Validation (7 Days)
1. Accumulate 7 days of picks with all-horses data
2. Run full comparative analysis
3. Review pattern recommendations
4. Validate insights are actionable and specific

---

## üìà Expected Outcomes

### Short-term (First Week):
- ‚úÖ All horses stored per race (not just selections)
- ‚úÖ Comparative analysis identifies specific mistakes
- ‚úÖ Recommendations become actionable ("reduce X by Y%")

### Medium-term (First Month):
- üìä Pattern detection improves (larger sample size)
- üéØ Systematic biases identified (form overweight, class underweight)
- üîß Scoring adjustments quantified

### Long-term (3+ Months):
- üß† Self-correcting system: detects own biases
- üìà Win rate improves through targeted recalibration
- üéì Continuous learning from winner comparisons

---

## üöÄ Future Enhancements

### V2.4 Possibilities:
1. **Auto-Calibration:** Automatically adjust expert weights based on patterns
   - If form overweight detected 3 weeks straight ‚Üí reduce form_score multiplier
   
2. **Winner Prediction:** Analyze all horses after race to see if winner showed signals
   - "Pal Joey had course_wins=3, we only weighted course_wins at 10%, should be 25%"
   
3. **Odds Comparison:** Compare AI probabilities vs market odds for ALL horses
   - Find systematic value in horses we didn't pick
   
4. **Ensemble Re-weighting:** Adjust value/form/class expert influence dynamically
   - Value expert: Currently 33% ‚Üí Adjust to 40% if consistently accurate

---

## üìù Code Changes Summary

| File | Changes | Lines Modified |
|------|---------|----------------|
| enhanced_analysis.py | Added `all_horses` array to 3 expert prompts, collect in analyze_race_enhanced | ~60 lines |
| save_selections_to_dynamodb.py | Added `all_horses_analyzed` field to bet_item | ~5 lines |
| analyze_prediction_calibration.py | Added `compare_pick_vs_winner()` and `analyze_all_losses_vs_winners()` | ~200 lines |

**Total:** ~265 lines added  
**Complexity:** Medium (new data structure + analysis functions)  
**Risk:** Low (additive changes, no modification to existing logic)

---

## üéâ Success Metrics

### Immediate (Today):
- [x] Code implemented without errors
- [x] Prompts updated to request all horses
- [x] Storage schema expanded
- [x] Comparison functions created

### Next Workflow Run:
- [ ] `all_horses_analyzed` field populated in database
- [ ] All 3 expert analyses present
- [ ] All runners in race included in data

### After First Settlements:
- [ ] `loss_comparison_analysis.json` generated successfully
- [ ] Comparative insights show specific patterns
- [ ] Recommendations are actionable

### After 7 Days:
- [ ] Pattern analysis identifies systematic biases
- [ ] Learning insights more specific than V2.2
- [ ] Calibration recommendations quantified

---

**Implementation Status:** ‚úÖ READY FOR TESTING  
**Next Step:** Run scheduled workflow and verify all-horses data capture  
**Documentation:** Updated in BETTING_STRATEGY_V2.md

---

*This enhancement transforms the learning system from "we were wrong" to "here's exactly WHY we were wrong and HOW to fix it."*
